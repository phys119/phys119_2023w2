{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0607874-fa9a-44d1-a7af-2c5db485891a",
   "metadata": {},
   "source": [
    "# Lab 08 Prelab: Analytic chi-squared minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b70d5-9afc-45b0-bd6a-56ba39311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import data_entry2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c7a22-ebd3-4977-9d87-395137731c65",
   "metadata": {},
   "source": [
    "In Lab 08 you will continue to collect data from the RC circuit in an effort to improve your dataset. Additionally, this prelab will provide you with an analytic formula to help you automatically **calculate** the best fitting parameters to your data.\n",
    "\n",
    "**An important reminder: the \"best fit\" is not the same as a \"good fit\".** Even though the analytic formula will give you the best fit of your model to your data, it does not allow you to say \"this is a *good* fit\". The best fit, by definition, is one that minimizes chi-squared, but a minimized chi-squared of $\\chi^2=11.8$ or one with a strong pattern in the residuals is still not considered a good fit to your data. Conversely, a minimized chi-squared of $\\chi^2=0.05$ still requires you to take a closer look at your uncertainty estimation strategy, since the uncertainties would likely be overestimated. This automated method of chi-squared minimization still requires you to use residuals plots and your other techniques for assessing goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d44b16-605b-4d2c-b5f6-a5679404bc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An analytic formula for minimizing chi-squared for a one-parameter model, $y = mx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ee9a5-d5b5-47bf-b100-b396faa1bc8e",
   "metadata": {},
   "source": [
    "Recall that chi-squared is a continuous function of the fitting parameters in your model. Specifically, if you have $P$ parameters in your model, then chi-squared is a $P$-dimensional function. For instance, if we are fitting a one-parameter linear model, $y=mx$, then $m$ is the sole parameter and the associated chi-squared function is $\\chi^2(m)$. For a two-parameter linear model, $y=mx+b$, then we would have $\\chi^2(m,b)$. \n",
    "\n",
    "For simple model functions like straight lines, we can use calculus to analytically find the best values of the parameters without having to iteratively search for them. Below, we'll work through that calculation for the case of a straight line fit with no intercept. It is not crucial that you understand all the steps here.\n",
    "\n",
    "From your calculus courses, you know that you can take a continuous function $f(x)$ and find the minimum or maximum (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and solving the resulting equation\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can do exactly this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data that is finite, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse to the data, meaning that chi-squared has no maxima. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best fit slope\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{u\\_y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right)^2 \\right] = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0ea8-14ed-41f6-bc95-eb3aa7aee936",
   "metadata": {},
   "source": [
    "*If we differentiate the above and solve for $m$, we will find the slope ($m$) that corresponds to the lowest possible chi-squared. Here, we skip ahead to the solution, but include the full derivation in an appendix at the end of this document.*\n",
    "\n",
    "$$ m = \\frac{\\sum_{i=1}^N  \\frac{x_iy_i}{(u\\_y_i)^2}}{\\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2}} $$\n",
    "\n",
    "So given our $x$ and $y$ data plus the uncertainty in $y$, we are able to analytically solve for the best fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ u\\_m = \\sqrt{\\frac{1}{\\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2}}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8a627-0ea4-46a4-85da-1ec6cde33ba8",
   "metadata": {},
   "source": [
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2}$ appears both in $m$ and $u\\_m$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2} $$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ m = \\frac{1}{Z} \\sum_{i=1}^N  \\frac{x_iy_i}{(u\\_y_i)^2} $$\n",
    "$$ u\\_m = \\sqrt{\\frac{1}{Z}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d610e4-5217-4038-8147-555422212c01",
   "metadata": {},
   "source": [
    "**Your turn #1:** Take a close look at the expression for $u\\_m$. How do (a) the number of data points, $N$, and (b) the uncertainies in the data, $u\\_y_i$, impact the relative uncertainty $u\\_m/m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad46f59-2db0-4447-acea-cd032c4b3553",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17e81fb5-c900-4b8d-ae55-3d292ffcc10f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applying the analytic equation to sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc96ecf-5204-4bc9-b413-62a88b0db571",
   "metadata": {},
   "source": [
    "The code below does some of the work to set up these equations for $m$ and $u\\_m$ for the sample data of last week's pre-lab. \n",
    "\n",
    "**Your turn #2:** Work your way through the code below and <u>complete the missing steps</u>, and then run the cell to see:\n",
    "\n",
    "- A plot of the data and best fit slope $m$, along with lines for those at the 68% CI limits: ($m+u\\_m$), and ($m-u\\_m$); and\n",
    "- the corresponding residuals plots;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c920f5-c74b-48e5-960c-236a4ddcdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the missing steps in this cell and run this cell\n",
    "\n",
    "\"\"\" Sample data \"\"\"\n",
    "xdata = np.array([0.1, 0.16428571, 0.22857143, 0.29285714, 0.35714286, 0.42142857, 0.48571429, 0.55, 0.61428571, 0.67857143, \\\n",
    "                   0.74285714, 0.80714286, 0.87142857, 0.93571429, 1.])\n",
    "ydata = np.array([0.33336864, 0.5414786, 0.82003978, 1.09858314, 1.27560974, 1.52025082, 1.67681586, 2.03833678, \\\n",
    "                  2.35943739, 2.36120224, 2.74941308, 2.83963194, 2.9932707, 3.40978616, 3.44578725])\n",
    "u_ydata = np.array([0.01666843, 0.02707393, 0.04100199, 0.05492916, 0.06378049, 0.07601254, 0.08384079, 0.10191684, \\\n",
    "                   0.11797187, 0.11806011, 0.13747065, 0.1419816, 0.14966353, 0.17048931, 0.17228936])\n",
    "\n",
    "\"\"\" Find the best model corresponding to the minimized chi-squared \"\"\"\n",
    "\n",
    "# calculations for the analytic best fit here: \n",
    "Z = np.sum( (xdata * xdata) / u_ydata**2) # calculate sum(x_i*x_i/(u_y_i)^2)\n",
    "\n",
    "### FILL IN THESE TWO MISSING STEPS (calculate m and u_m) ###\n",
    "\n",
    "# Calculate best fit slope\n",
    "# m = \n",
    "\n",
    "# Calculate uncertainty in best fit slope\n",
    "# u_m = \n",
    "\n",
    "# Print the best fit slope and uncertainty\n",
    "print(\"Best fit slope m = \", m, \"±\", u_m)\n",
    "\n",
    "# Find max and min slopes based on m and u[m]\n",
    "mMax = m + u_m # slope at maximum of 68% CI \n",
    "mMin = m - u_m # slope at minimum of 68% CI \n",
    "\n",
    "\"\"\" Construct the models for plotting; calculate residuals \"\"\"\n",
    "\n",
    "ymodelBest = m * xdata # best fit model\n",
    "ymodelMax = mMax * xdata # max model\n",
    "ymodelMin = mMin * xdata # min model\n",
    "\n",
    "res = ydata - ymodelBest # calculate residuals (best fit)\n",
    "wres2 = (res/u_ydata)**2 # weighted residuals squared\n",
    "    \n",
    "\"\"\" Calculate chi-squared \"\"\"\n",
    "    \n",
    "N = len(xdata) # number of data points\n",
    "P = 1 # number of parameters\n",
    "chi2 = np.sum(wres2) / (N - P) # calculate chi-squared\n",
    "print(\"chi2 = {:.4f}\".format(chi2))\n",
    "\n",
    "\"\"\" Plot data and fits \"\"\"\n",
    "\n",
    "plt.errorbar(xdata, ydata, u_ydata, marker='.', linestyle='', color='k')\n",
    "plt.plot(xdata, ymodelBest, label=\"best fit\")\n",
    "plt.plot(xdata, ymodelMax, label=\"max fit\")\n",
    "plt.plot(xdata, ymodelMin, label=\"min fit\")\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('y data')\n",
    "plt.title('Data with fit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Plot residuals for the best fit \"\"\"\n",
    "\n",
    "plt.errorbar(xdata, res, u_ydata, marker='.', linestyle='')\n",
    "plt.hlines(y=0, xmin=np.min(xdata), xmax=np.max(xdata), color='k') # draw axis at y = 0.\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals plot (best fit, $\\chi^2$={:.4f})'.format(chi2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076950fe-1591-441e-9a0c-23e8e735ad36",
   "metadata": {},
   "source": [
    "### Discussion of goodness of fit using the residuals plot and chi-squared\n",
    "\n",
    "You should find the best fit slope is 3.545 ± 0.046, corresponding to chi-squared = 0.76. \n",
    "\n",
    "Let's take a moment to ask ourselves if this is a good fit. \n",
    "\n",
    "If we look first at the residuals, we see that there are no obvious trends and the residuals seem to be distributed randomly and evenly above and below the Residuals = 0 line. We also see an appropriate number of the residual uncertainties crossing this Residuals = 0 line. The residuals graph suggests we have a pretty good fit. This is further reinforced by our chi-squared of 0.76, which also suggests we have a good fit. These two pieces of evidence allow us to conclude that, overall, we have a good fit of this model to these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c6eb3-060f-4ffa-bd57-0e0f8f989ca9",
   "metadata": {},
   "source": [
    "## Visualizing chi-squared minimization\n",
    "\n",
    "Below, you can see a visualization of how the analytically-determined $m$ lies at the lowest point of the chi-squared vs. $m$ curve (as expected for a minimum). An array of many different $m$ values is created, and chi-squared is calculated for each of those different slope values. Finally, the calculated chi-squared values are plotted versus slope.\n",
    "\n",
    "The code below uses a programming construction of python called a \"for\" loop. You will not need to know how to use these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d8540-6a0a-469d-8f0b-a1c3462aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to see a plot of chi-squared vs slope, for slopes ranging from mMin to mMax\n",
    "\n",
    "\"\"\" plot chi-squared as a function of m \"\"\"\n",
    "\n",
    "mVec = np.linspace(mMin,mMax,100) # prepare an array of different slope values between mMin and mMax.\n",
    "chi2Vec = np.zeros(np.size(mVec)) # create an array of chi-squared values, set each to 0 for now.\n",
    "\n",
    "for i in range(len(mVec)): # loop through all the different m values.\n",
    "    # the indented code below is executed once for each of the m values \n",
    "    # we calculate chi-squared for each possible slope.\n",
    "    ymodelTemp = mVec[i]*xdata # model for the current value of m in the vector\n",
    "    resTemp = ydata - ymodelTemp # residuals for this model\n",
    "    wres2Temp = (resTemp / u_ydata)**2 # weighting these residuals\n",
    "    chi2Vec[i] = np.sum(wres2Temp) / (N - P) # store chi2 in the i'th position of chi2Vec.\n",
    "    \n",
    "plt.plot(mVec, chi2Vec, 'k')\n",
    "plt.plot(m, chi2, 'o', label='best fit')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Chi-squared')\n",
    "plt.title('Visualizing chi-squared minimization in chi-squared space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a58820-221e-46dd-9675-9052f2e14c0b",
   "metadata": {},
   "source": [
    "## Prepare for Lab 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd5f7-2f56-4829-bb14-58187c9acd32",
   "metadata": {},
   "source": [
    "In Lab 08, you will use the analytic formula derived above to calculate the best fit slope for your time-constant versus resistance data. \n",
    "\n",
    "**Your turn #3:** In preparation for Lab 08, adapt your calculations from above for your Lab 07 data. Check that this gets you a lower chi-squared than what you got from minimizing by hand in Lab 07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89559f10-928a-4540-8bd8-5ae30f1f0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to re-analyze your Lab 07 data using your \n",
    "# python calculations from the analytic formula from above\n",
    "\n",
    "# You will need to adjust the source filename below to match the name of your Lab 07 data file:\n",
    "de = data_entry2.sheet_copy('../Lab07/lab07data2','lab07datacopy')\n",
    "\n",
    "# Now calculate Z, m, and u_m\n",
    "\n",
    "# Calculate chi-squared with calculated m. \n",
    "\n",
    "# It's a good idea to produce the relevant plots (scatter plot with model, and residuals plot) as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afeee-7cb1-4807-8668-e30d7abb0d85",
   "metadata": {},
   "source": [
    "# Appendix - The full derivation of the analytic formula for minimizing chi-squared for a one-parameter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde226e-3b00-451d-ae48-be71f10f04bc",
   "metadata": {},
   "source": [
    "From your calculus courses, you know that you can take a continuous function $f(x)$ and find the minimum or maximum (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and solving the resulting equation\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can do exactly this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data that is finite, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse to the data, meaning that chi-squared has no maxima. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best fit slope\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{u\\_y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Since the derivative is with respect to $m$, it has no effect on $N$ or $P$, meaning we can move that leading fraction outside of the derivative.\n",
    "\n",
    "$$ \\frac{1}{N-P} \\frac{d}{dm} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right)^2 = 0.$$\n",
    "\n",
    "The summation is only over variables with a subscript \"$i$\"; $m$ does not contain this so we can also switch the order of differentiation and summation\n",
    "\n",
    "$$ \\frac{1}{N-P} \\sum_{i=1}^N \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right)^2 = 0.$$\n",
    "\n",
    "Now we perform some calculus and take the derivative (invoking the chain rule)\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right) \\cdot \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right)= 0,$$\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right) \\cdot \\left(-\\frac{x_i}{u\\_y_i}\\right) = 0.$$\n",
    "\n",
    "The negative sign can be taken outside the sum, and since we are setting everything equal to zero the $2/(N-P)$ can be discarded\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{u\\_y_i}\\right) \\cdot \\frac{x_i}{u\\_y_i} = 0.$$\n",
    "\n",
    "What remains is to rearrange this expression for $m$. We can start by expanding the terms in the summation\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i}{u\\_y_i} - m\\frac{x_i}{u\\_y_i}\\right) \\cdot \\frac{x_i}{u\\_y_i} = 0$$\n",
    "$$ \\sum_{i=1}^N  \\frac{x_iy_i}{(u\\_y_i)^2} - m\\frac{x_i^2}{(u\\_y_i)^2} = 0$$\n",
    "\n",
    "then finally isolate $m$\n",
    "\n",
    "$$ m = \\frac{\\sum_{i=1}^N  \\frac{x_iy_i}{(u\\_y_i)^2}}{\\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2}} $$\n",
    "\n",
    "So given our $x$ and $y$ data plus the uncertainty in $y$, we are able to analytically solve for the best fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ u\\_m = \\sqrt{\\frac{1}{\\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2}}} .$$\n",
    "\n",
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2}$ appears both in $m$ and $u\\_m$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{x_i^2}{(u\\_y_i)^2} $$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ m = \\frac{1}{Z} \\sum_{i=1}^N  \\frac{x_iy_i}{(u\\_y_i)^2} $$\n",
    "$$ u\\_m = \\sqrt{\\frac{1}{Z}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193db82c-8f80-4c7b-8c06-3323a4d98aa0",
   "metadata": {},
   "source": [
    "# Submit\n",
    "\n",
    "Steps for submission:\n",
    "\n",
    "1. Click: Run => Run_All_Cells\n",
    "2. Read through the notebook to ensure all the cells executed correctly and without error.\n",
    "3. File => Save_and_Export_Notebook_As->HTML\n",
    "4. Inspect your exported html file.\n",
    "5. Upload the HTML document to the lab submission assignment on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befbf55b-ac43-4216-9726-54bbb707d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sheets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
